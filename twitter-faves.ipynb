{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering (WIP) my twitter likes\n",
    "\n",
    "I entered Twitter because of the network of researchers and programmers that posts there. Because of that, I also have been doing 'favs' quite selectively, with an occasional like on some anime or cat or political tweet. As a \"data scientist\" (I almost choked saying that) I want to cluster the favorite tweets based on the text presented in these tweets - I reckon that it could be necessary to scrap the content that some of these tweets link to, but I am keeping it simple because I only faved what met the eye to be honest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the twitter api\n",
    "\n",
    "The first step is to go to the 'developers' section on twitter and create an app for personal use. This is to obtain the credentials to connect to the api - it's kind of like username/password but slightly more complicated. Instead of coding http requests from scratch, we are going to use an open source api wrapper, found on [bear/python-twitter](https://github.com/bear/python-twitter). After following the installation you can import the module as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import twitter # has to be installed with 'pip install python-twitter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys # so as to know the error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My API credentials are not explicit here, as this is in the open web. Note that there is a running joke that you can find free api keys on github (for services that are paid!). This is a read-in from a `.gitignore`d file, but you can imagine that these api keys look like something your cat would type on the keyboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('creds.pkl', 'rb') as handle:\n",
    "    creds = pickle.load(handle)\n",
    "\n",
    "# connect to the twitter api with your twitter app credentials\n",
    "api = twitter.Api(consumer_key = creds['consumer_key'],\\\n",
    "                  consumer_secret = creds['consumer_secret'],\\\n",
    "                  access_token_key = creds['access_token_key'],\\\n",
    "                  access_token_secret = creds['access_token_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with this last call, we are connected to the api that wraps our http requests into functions :). I can then proceed to fetch some data. After sifting through some of the documentation of the api wrapper, I found what I wanted: the [`GetFavorites()`](https://python-twitter.readthedocs.io/en/latest/twitter.html#twitter.api.Api.GetFavorites) method that wraps the http request to the corresponding [endpoint](https://developer.twitter.com/en/docs/tweets/post-and-engage/api-reference/get-favorites-list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recent_favs = api.GetFavorites(screen_name='burnie093', count = 200) # this is a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Steps to better code\\n{ author: @isaacandsuch }\\nhttps://t.co/lHL37fyC8e'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recent_favs[0].AsDict() # recent_favs[*] is a twitter.models.Status with a method AsDict()\n",
    "recent_favs[0].AsDict()['text'] # you can now access the dict element text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the texts to a file\n",
    "\n",
    "Part of the drill is to save data to files to later read it back in. And note, we only have the most recent 200 tweets, and we want to get all of the ~900 favorited tweets. Also note this rule of thumb, if you're getting data > 100 MB, you should use a database engine (sql or nosql). Of course, for this text data, this would be ovekill! So saving it to files will suffice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formats\n",
    "Ok here are possible formats to save the file\n",
    "- `json` (\"JavaScript Object Notation\"), this is a stringified version of the notation every js object uses and can be seen as correspondent to the python dict. Just a mention so that you know what this means when reading other documentation.\n",
    "- plain text or csv, need I say more? It's a classic one and often used as well. But I want to leverage a python/c feature so I'll go with the next\n",
    "- **`pickle`** this is a serialization mechanism to save a **python object** such as the list we currently have stored in the variable `texts`. Unlike the other two formats, this reads in the data as a python object, saving me from dealing with type conversion. [Here](https://stackoverflow.com/a/11218504)'s how to use it.\n",
    "\n",
    "And for future reference for beginners, a file is just really a file which is not determined by its extension, only by its content. Extensions are just conventions among us computer people. If you want to give some wings to your evil genius, you could write a python program and save it with a java extension, e.g. `definitely_not_python.java`, and the fact is, the command `python definitely_not_python.java` (executing the python program) would work because the syntax would be parsed as python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_save = [recent_favs[i].AsDict() for i in range(len(recent_favs))] # I changed my mind\n",
    "with open('data/favs_0.pkl', 'wb') as handle: # 'wb' stands for 'write binary'\n",
    "    pickle.dump(to_save, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read it back in again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = []\n",
    "for i in range(5):\n",
    "    with open('data/favs_%d.pkl' % i, 'rb') as handle:\n",
    "        batches.append(pickle.load(handle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': 'Sat Sep 30 08:02:01 +0000 2017',\n",
       " 'favorite_count': 62,\n",
       " 'favorited': True,\n",
       " 'hashtags': [],\n",
       " 'id': 914037602378010624,\n",
       " 'id_str': '914037602378010624',\n",
       " 'lang': 'en',\n",
       " 'retweet_count': 18,\n",
       " 'source': '<a href=\"http://bufferapp.com\" rel=\"nofollow\">Buffer</a>',\n",
       " 'text': 'Steps to better code\\n{ author: @isaacandsuch }\\nhttps://t.co/lHL37fyC8e',\n",
       " 'urls': [{'expanded_url': 'https://dev.to/isaacandsuch/steps-to-better-code',\n",
       "   'url': 'https://t.co/lHL37fyC8e'}],\n",
       " 'user': {'created_at': 'Fri Aug 15 19:11:17 +0000 2014',\n",
       "  'description': 'Coding resources, commentary and community. Helping you become a better developer maybe. Created by @bendhalpern',\n",
       "  'favourites_count': 40814,\n",
       "  'followers_count': 128115,\n",
       "  'following': True,\n",
       "  'friends_count': 2253,\n",
       "  'id': 2735246778,\n",
       "  'lang': 'en',\n",
       "  'listed_count': 2658,\n",
       "  'name': 'The Practical Dev',\n",
       "  'profile_background_color': '131516',\n",
       "  'profile_background_image_url': 'http://pbs.twimg.com/profile_background_images/601385812760993793/2pxLSwZZ.jpg',\n",
       "  'profile_background_tile': True,\n",
       "  'profile_banner_url': 'https://pbs.twimg.com/profile_banners/2735246778/1492833420',\n",
       "  'profile_image_url': 'http://pbs.twimg.com/profile_images/881896578633445376/j_dhfgoj_normal.jpg',\n",
       "  'profile_link_color': '14BD7B',\n",
       "  'profile_sidebar_fill_color': '000000',\n",
       "  'profile_text_color': '000000',\n",
       "  'screen_name': 'ThePracticalDev',\n",
       "  'statuses_count': 11386,\n",
       "  'time_zone': 'Pacific Time (US & Canada)',\n",
       "  'url': 'https://t.co/lhcCPP1ReQ',\n",
       "  'utc_offset': -25200},\n",
       " 'user_mentions': [{'id': 302340509,\n",
       "   'name': 'Isaac Lyman',\n",
       "   'screen_name': 'isaacandsuch'}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches[0][0]  # this is what an instance looks like... It's still json data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting all the faved tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there is a maximum count of the tweets you can retrieve from twitter: 200. So for each request, we can retrieve 200 tweets at a time. Let us collect the remaining ones, and for such we need to know the `max_id` to pass to the function. It should be at the last element..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "896645982854754304"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxi = hi[-1]['id']\n",
    "maxi # 896645982854754304"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loops the store of all my faved tweets so far, and because the list is finite, it will throw an error when it can't retrieve any more tweets. And to Marcel: **you don't need to touch this, the data is stored already** ^\\_^ The data is in files `favs_0.pkl` through `favs_4.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxi = 896645982854754304\n",
    "for f in range(1,10): # will do this four more times, and will probably throw an error at some point\n",
    "    try:\n",
    "        favs = api.GetFavorites(screen_name='burnie093', count = 200, max_id = maxi)\n",
    "        to_save = [favs[i].AsDict() for i in range(len(favs))]\n",
    "        with open('favs_%d.pkl' % f, 'wb') as handle: # 'wb' stands for 'write binary'\n",
    "            pickle.dump(to_save, handle)\n",
    "        \n",
    "        # prepare for the next iteration\n",
    "        f += 1\n",
    "        if maxi == to_save[-1]['id']:\n",
    "            break\n",
    "        else:\n",
    "            maxi = to_save[-1]['id']\n",
    "    except:\n",
    "        print(sys.exc_info()[0])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('favs_4.pkl', 'rb') as handle:\n",
    "    some = pickle.load(handle)\n",
    "len(some)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and extracting only text\n",
    "By the way, let me drop this thing on more [nested list comprehension](https://stackoverflow.com/a/8050243), as probably the following could have been one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "844"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = []\n",
    "for i in range(5): # i belongs to [0, 5[\n",
    "    with open('data/favs_%d.pkl' % i, 'rb') as handle:\n",
    "        texts.extend([t['text'] for t in pickle.load(handle)])\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that corresponds to the same number of faved tweets on my twitter profile, except ahead of the website server by 2 faved tweets? ![](./data/faved.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally! Oh wait, now we clean the data\n",
    "\n",
    "Most of this data has really weird characters... For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Steps to better code\\n{ author: @isaacandsuch }\\nhttps://t.co/lHL37fyC8e'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And actually, I think only the first part would be interesting. We could keep the author... to see if I follow more of the same author.. And by the way, big coincidence that this is from dev.to! The first step is to remove special characters such as `\\n`. This is an escape character `\\` followed by a letter to issue a command, `n`, which in this case corresponds to a new line. This is obviously programmed by the dudes and dudettes from dev.to to programatically compose the tweet in a nice manner, which you cannot see on the browser or app as `\\n` is basically a rendering instruction.\n",
    "\n",
    "About links... Since these can be found in the pickle files mapped to key `link` or similar, it does not seem interesting to keep them here for the sake of this one analysis. So we need a regex to remove `https://t.co*` until it finds a space or any other character. And my god do I fear regex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of it, I will show some faved tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Steps to better code\\n{ author: @isaacandsuch }\\nhttps://t.co/lHL37fyC8e',\n",
       " 'I just published “Understanding your energy bill” https://t.co/Q2MutjiFjE',\n",
       " 'Yes I am making sure adequate cat photo benchmarks are in our talk and am currently comparing the performance of different cats',\n",
       " 'Coding = thinking in several dimensions\\n{ author: @andreasklinger }\\nhttps://t.co/3WHV4x6XEZ',\n",
       " \"@pewdiepie Kimi no na wa used heavily japanese ideas in the story that was pretty vital to the plot. Hollywood's go… https://t.co/OPFp64DLHg\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data\n",
    "The following just is very inspired by this [post](https://machinelearningmastery.com/clean-text-machine-learning-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Little curiosity\n",
    "One of the following steps is to remove \"stop\" words, words that are often auxiliary in constructing meaning, but lack content. You may itch to know what these words can be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/burnie/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk; nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "['de', 'a', 'o', 'que', 'e', 'do', 'da', 'em', 'um', 'para']\n",
      "['aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english')[:10])\n",
    "print(stopwords.words('portuguese')[:10])\n",
    "print(stopwords.words('german')[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following defines how the tweets are cleaned. This is kind of naïve but it should do the job for a simple demo analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    # word tokenizer\n",
    "    tokenizer = TweetTokenizer(strip_handles=False)\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    # lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # remove non-alphabetic tokens, such as punctuation, urls.\n",
    "    # a couple of 'or's are there to include further content such as @rustlang or #haskell\n",
    "    tokens = [token for token in tokens if token.isalpha() or 'lang' in token or '#' in token]\n",
    "    # however the hash # may be part of a url instead...\n",
    "    # remove stop words\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    # return\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as a token of example, here goes an example how a raw tweet is cleaned. If you're running this notebook yourself, you can notice how this can be so aggressive in removing subjective meaning and personal tastes (try  `sample = 24` for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just published “Understanding your energy bill” https://t.co/Q2MutjiFjE\n",
      "--------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['published', 'understanding', 'energy', 'bill']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 1\n",
    "print(texts[sample])\n",
    "print('--------------')\n",
    "clean_tweet(texts[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're are havin' a ball with @haskell_lang #lambdapt #meetup https://t.co/h4wWYx7PBn\n",
      "--------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['havin', 'ball', '@haskell_lang', '#lambdapt', '#meetup']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 24\n",
    "print(texts[sample])\n",
    "print('--------------')\n",
    "clean_tweet(texts[sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyways, collect the clean data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = [clean_tweet(tweet) for tweet in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['steps', 'better', 'code', 'author']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('new', 45),\n",
       " ('like', 40),\n",
       " ('people', 37),\n",
       " ('learning', 35),\n",
       " ('de', 34),\n",
       " ('get', 32),\n",
       " ('want', 32)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctr = Counter()\n",
    "for clean_tweet in cleaned:\n",
    "    ctr.update(clean_tweet)\n",
    "ctr.most_common(7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
